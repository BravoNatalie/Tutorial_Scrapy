{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images\scrapy_tutorial.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "O [Scrapy](https://scrapy.org/) é um framework de código aberto escrito em python para web crawling e web scraping, em que deseja-se extrair dados estruturados de fontes não estruturadas, como páginas web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling & Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web crawlers são bots, spider bots, que percorrem páginas web indexando-as. Além poderem ser usados para fazer extração de dados dessas páginas, o que é chamado de web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Web-Crawling.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste tutorial vamos aprender o funcionamento básico do Scrapy.<br>\n",
    "Para isso utilizaremos um simples exemplo, que consiste em retirar algumas informações sobre os 50 animes mais populares do site [MyAnimeList](https://myanimelist.net/topanime.php?type=bypopularity) e armazená-las em um arquivo JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/mal-populars.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando o Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Scrapy é compatível com as duas versões do Python. Se você estiver usando o Anaconda ou o Miniconda, poderá instalar o pacote do canal Conda-Forge, que tem pacotes atualizados para Linux, Windows e OS X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar o Scrapy usando conda, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso contrário, se estiver usando Linux ou Mac OS X, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente é necessário configurar um novo projeto Scrapy. Para isso abra o terminal no diretório em que deseja salvar o código e execute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject scrapy_anime_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta forma obtemos a pasta <font color=\"green\">scrapy_anime_data</font> com os seguintes elementos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/estrutura.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de itens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para especificar os dados que queremos extrair é necessário definir no arquivo <font color=\"green\">items.py</font> uma classe que irá conter os diferentes dados para cada item extraído. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/items.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiders são classes que definem como um grupo de sites terá seus dados extraídos. Elas necessitam de:\n",
    "\n",
    "<ul>\n",
    "    <li> nome; \n",
    "    <li> lista de URLs, que a spider irá percorrer inicialmente;\n",
    "    <li> método <b>parse( )</b>,que analisa cada resposta baixada (páginas), extrai os dados e encontra novas URLs.\n",
    "</ul>\n",
    "\n",
    "O arquivo definindo as spiders deve ser criado na pasta <font color=\"green\">spiders/</font>. Para este tutorial criaremos um arquivo vazio chamado <font color=\"green\">anime_data_spider.py</font> dentro da pasta especificada anteriormente.<br>\n",
    "<br>\n",
    "No nosso arquivo devemos primeiramente importar algumas funções do scrapy e o nosso modelo de itens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy_anime_data.items import Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois é preciso estabelecer o nome da classe de nossa spider, assim como o nome que será usado pelo scrapy para chamá-la e a lista de URLs que iremos iniciar a extração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataSpider(scrapy.Spider):\n",
    "    name = \"anime_data\"\n",
    "    start_urls = [\"https://myanimelist.net/topanime.php?type=bypopularity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, dizemos ao scrapy como deve ser analisada cada página web visitada através da função <b>parse()</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta função utilizaremos a linguagem [XPath](https://en.wikipedia.org/wiki/XPath) para caminhar pela página HTML retornada pelo parâmetro <i>response</i>.<br>\n",
    "Para facilitar o entendimento verifique como é feito o caminhamento em XPath no exemplo abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/html_xpath.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja agora como podemos caminhar pela página que iremos extrair os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/mal_inspect.png\" width=\"900\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O XPath nos retorna o resultado como uma lista em formato HTML, logo se quisermos o texto contido no retorno é preciso usar a função <b>text()</b> e extrair o resultado da lista. Após a obtenção dos dados requeridos podemos armazená-los no modelo de itens anteriormente criado.<br>\n",
    "<br>\n",
    "Por fim, o nosso método <b>parse()</b> ficará como a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        animes = response.xpath(\"//tr[@class='ranking-list']\")\n",
    "        for anime in animes:\n",
    "            rank = Rank()\n",
    "            rank['position'] = anime.xpath('td[1]//span//text()').extract_first()\n",
    "            rank['name'] = anime.xpath('td[2]//div[@class=\"di-ib clearfix\"]//a//text()').extract_first()\n",
    "            rank['score'] = anime.xpath('td[3]//div[@class=\"js-top-ranking-score-col di-ib al\"]//span//text()').extract_first()\n",
    "            yield rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando o spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Scrapy pode armazenar os dados em vários formatos diferentes e iremos utilizar o formato JSON.<br>\n",
    "Para executar o spider basta abrir o terminal dentro da pasta principal do projeto e efetuar o comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl anime_data -o result.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>OBS.:</b>\n",
    "<ul>\n",
    "    <li><font color=\"green\">anime_data</font> é o nome do spider; \n",
    "    <li><font color=\"green\">-o</font> especifica qual o nome do arquivo e formato que será salvo o resultado.\n",
    "</ul>\n",
    "\n",
    "Quando o processo de extração terminar o arquivo <font color=\"green\">result.json</font> estará dentro na pasta <font color=\"green\">scrapy_anime_data/scrapy_anime_data/</font> com o resultado a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/resultado.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o Scrapy ainda é possível:\n",
    "<ul>\n",
    "    <li> Persistir o resultado em um banco de dados;\n",
    "    <li> Rodar o spider periodicamente com o Scrapy Cloud.\n",
    "</ul>\n",
    "\n",
    "Código disponível em: https://github.com/BravoNatalie/Tutorial_Scrapy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponível em: https://docs.scrapy.org/en/latest/intro/tutorial.html acessado em 3 de Outubro de 2018.<br>\n",
    "Disponível em: https://www.youtube.com/watch?v=rj8Sqsgh5TM&t=168s acessado em 4 de Outubro de 2018.<br>\n",
    "Disponível em: https://www.youtube.com/watch?v=DRfEY-qzyvk&t=311s acessado em 4 de Outubro de 2018.<br>\n",
    "Disponível em: https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html acessado em 5 de Outubro de 2018.<br>\n",
    "Disponível em: https://stair.wm.edu/Webscraping2_scrapy.html acessado em 6 de Outubro de 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
